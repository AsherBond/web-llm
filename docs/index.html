<!DOCTYPE html>
<html>
    <head>
        <title>WebLLM | Home</title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css"
              integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
              crossorigin="anonymous">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
        <link rel="stylesheet" href="/assets/css/main.css">
        <link rel="stylesheet" href="/assets/css/group.css">
        <!-- <link rel="stylesheet" href="/css/table.css">          -->
        <link rel="shortcut icon" href="/assets/img/logo/mlc-favicon.png">
        <meta http-equiv="origin-trial" content="Agx76XA0ITxMPF0Z8rbbcMllwuxsyp9qdtQaXlLqu1JUrdHB6FPonuyIKJ3CsBREUkeioJck4nn3KO0c0kkwqAMAAABJeyJvcmlnaW4iOiJodHRwOi8vbG9jYWxob3N0Ojg4ODgiLCJmZWF0dXJlIjoiV2ViR1BVIiwiZXhwaXJ5IjoxNjkxNzExOTk5fQ==">
<meta http-equiv="origin-trial" content="AnmwqQ1dtYDQTYkZ5iMtHdINCaxjE94uWQBKp2yOz1wPTcjSRtOHUGQG+r2BxsEuM0qhxTVnuTjyh31HgTeA8gsAAABZeyJvcmlnaW4iOiJodHRwczovL21sYy5haTo0NDMiLCJmZWF0dXJlIjoiV2ViR1BVIiwiZXhwaXJ5IjoxNjkxNzExOTk5LCJpc1N1YmRvbWFpbiI6dHJ1ZX0=">
<script src="https://code.jquery.com/jquery-3.6.3.min.js" integrity="sha256-pvPw+upLPUjgMXY0G+8O0xUf+/Im1MZjXxxgOcBQBXU=" crossorigin="anonymous"></script>

    </head>
    <body>
        <div class="container">
            <!-- This is a bit nasty, but it basically says be a column first, and on larger screens be a spaced out row -->
            <div class="header d-flex
                        flex-column
                        flex-md-row justify-content-md-between">
              <a href="/" id="navtitle">
                  <img src="/assets/img/logo/mlc-logo-with-text-landscape.svg" height="70px"
                       alt="MLC" id="logo">
              </a>
              <ul id="topbar" class="nav nav-pills justify-content-center">
                    
                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link active"
                               href="/">
                                Home
                            </a>
                         
                        </li>

                    

                        

                        
                        
                        

                      <li class="nav-item">
                         
                            <a class="nav-link "
                               href="https://github.com/mlc-ai/web-llm">
                                Github
                            </a>
                         
                        </li>

                    

                </ul>
            </div>

            

            
            <!-- Schedule  -->
            
                <script>
  $(function(){
    $("#llm_chat").load("llm_chat.html");
  });
</script>

<h1 id="web-llm">Web LLM</h1>

<p><strong>Llama 2 7B/13B are now available in Web LLM!!</strong> Try it out in our <a href="#chat-demo">chat demo</a>.</p>

<p>This project brings large-language model and LLM-based chatbot to web browsers. <strong>Everything runs inside the browser with no server support and accelerated with WebGPU.</strong> This opens up a lot of fun opportunities to build AI assistants for everyone and enable privacy while enjoying GPU acceleration. Please check out our <a href="https://github.com/mlc-ai/web-llm">GitHub repo</a> to see how we did it.</p>

<p><img src="img/fig/pitts.png" width="100%" /></p>

<p>We have been seeing amazing progress in generative AI and LLM recently. Thanks to the open-source efforts like LLaMA, Alpaca, Vicuna and Dolly, we start to see an exciting future of building our own open source language models and personal AI assistant.</p>

<p>These models are usually big and compute-heavy. To build a chat service, we will need a large cluster to run an inference server, while clients send requests to servers and retrieve the inference output. We also usually have to run on a specific type of GPUs where popular deep-learning frameworks are readily available.</p>

<p>This project is our step to bring more diversity to the ecosystem. Specifically, can we simply bake LLMs directly into the client side and directly run them inside a browser? If that can be realized, we could offer support for client personal AI models with the benefit of cost reduction, enhancement for personalization and privacy protection. The client side is getting pretty powerful.</p>

<p>Won’t it be even more amazing if we can simply open up a browser and directly bring AI natively to your browser tab? There is some level of readiness in the ecosystem. This project provides an affirmative answer to the question.</p>

<h2 id="instructions">Instructions</h2>

<p>WebGPU just shipped to Chrome. You can try out the latest Chrome 113. Chrome version ≤ 112 is not supported, and if you are using it,
the demo will raise an error like <code class="language-plaintext highlighter-rouge">Find an error initializing the WebGPU device OperationError: Required limit (1073741824) is greater than the supported limit (268435456). - While validating maxBufferSize - While validating required limits.</code></p>

<p>Select the model you want to try out. Enter your inputs, click “Send” – we are ready to go!
The chat bot will first fetch model parameters into local cache. The download may take a few minutes, only for the first run.
The subsequent refreshes and runs will be faster. We have tested it on Windows and Mac, you will need a GPU with about 6GB memory to run Llama-7B, Vicuna-7B, and about 3GB memory to run RedPajama-3B.</p>

<p><strong>Some of the models requires fp16 support. To enable fp16 shaders, you will need to use the following instruction(<code class="language-plaintext highlighter-rouge">allow_unsafe_apis</code>) to turn on the support in Chrome Canary.</strong></p>
<ul>
  <li>Install <a href="https://www.google.com/chrome/canary/">Chrome Canary</a>, a Chrome nightly build for developers.</li>
  <li>Launch Chrome Canary from terminal with the following command:
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/Applications/Google\ Chrome\ Canary.app/Contents/MacOS/Google\ Chrome\ Canary --enable-dawn-features=allow_unsafe_apis
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="chat-demo">Chat Demo</h2>

<p>The chat demo is based on <a href="https://ai.meta.com/llama/">Llama 2</a>, <a href="https://huggingface.co/lmsys/vicuna-7b-delta-v1.1">vicuna-7b-v1.1</a> model and <a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1">RedPajama-INCITE-Chat-3B-v1</a> model. More model supports are on the way.</p>

<div id="llm_chat"></div>

<h2 id="links">Links</h2>

<ul>
  <li><a href="https://github.com/mlc-ai/web-llm">Web LLM Github</a></li>
  <li>You might also be interested in <a href="https://websd.mlc.ai/">Web Stable Diffusion</a>.</li>
</ul>

<h2 id="disclaimer">Disclaimer</h2>

<p>This demo site is for research purposes only, subject to the model License of LLaMA, Vicuna and RedPajama. Please contact us if you find any potential violation.</p>

            
        </div> <!-- /container -->

        <!-- Support retina images. -->
        <script type="text/javascript"
                src="/assets/js/srcset-polyfill.js"></script>
    </body>

</html>
